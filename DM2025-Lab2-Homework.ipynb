{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name: 張晴淳\n",
    "\n",
    "Student ID: 313700038\n",
    "\n",
    "GitHub ID: @JEMMA-CHANG\n",
    "\n",
    "Kaggle name: Ching-Chun-Chang  (team name: jj)\n",
    "\n",
    "Kaggle private scoreboard snapshot:  **rank-19**\n",
    "\n",
    "![pic_ranking.png](./pics/RANKING.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Syntax:** `##` creates a secondary heading (H2).\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "### **1.1 Preprocessing Steps**\n",
    "\n",
    "In this competition, three different input datasets were provided, and all of them had to be merged and cleaned before any model training or ensembling could take place:\n",
    "\n",
    "- **final_posts.json** – contains the full textual content of each post  \n",
    "- **emotion.csv** – contains emotion labels for training samples  \n",
    "- **data_identification.csv** – specifies whether a post belongs to the *train* or *test* subset (column name: **split**)  \n",
    "\n",
    "The preprocessing phase ensures that all information is properly integrated into a single, consistent dataset.\n",
    "\n",
    "#### **1.1.1 Data Loading**\n",
    "We first read the three raw files into pandas DataFrames:\n",
    "- `final_posts.json` → fields: *id*, *text*  \n",
    "- `emotion.csv` → fields: *id*, *emotion*  \n",
    "- `data_identification.csv` → fields: *id*, *split*  \n",
    "Loading them separately allows us to maintain each file’s original structure before merging.\n",
    "\n",
    "#### **1.1.2 Data Merging Workflow**\n",
    "All tables were merged using the shared key **`id`**, following a structured two-step process:\n",
    "1. **Merge post content with the split information**  \n",
    "   - This step combines the textual content with the indicator specifying whether each post is a *train* or *test* sample.  \n",
    "   - After merging, each row contains: post ID, text, and split label (`train` or `test`).\n",
    "2. **Add emotion labels**  \n",
    "   - We merge the label dataset using a *left join*, preserving all samples.  \n",
    "   - Training posts receive their correct emotion labels.  \n",
    "   - Test posts naturally remain unlabeled.\n",
    "After merging, the unified dataset includes:\n",
    "    - `id` – post ID  \n",
    "    - `text` – content of the post  \n",
    "    - `split` – `train` or `test`  \n",
    "    - `emotion` – available only for training samples  \n",
    "\n",
    "#### **1.1.3 Creating Training and Test Subsets**\n",
    "Using the **split** column, we divided the unified dataset into:\n",
    "    - **Training dataset** → contains text + emotion label  \n",
    "    - **Test dataset** → contains text + ID only  \n",
    "This separation allows model training on labeled data while holding out test samples for final prediction.\n",
    "\n",
    "#### **1.1.4 Splitting the Training Set Into Train/Validation**\n",
    "To evaluate model performance during development, we further partitioned the labeled training data into:\n",
    "    - **Training subset (~80%)**  \n",
    "    - **Validation subset (~20%)**\n",
    "We used a **stratified split**, ensuring that the distribution of the six emotion categories remains balanced across both subsets, preventing biased evaluation results.\n",
    "\n",
    "#### **1.1.5 Label Encoding**\n",
    "Transformer models require integer labels.  \n",
    "Therefore, we transformed each emotion category into numerical IDs by building two mappings:\n",
    "- **label → ID** (e.g., `anger → 0`, `fear → 1`, …) for training.\n",
    "- **ID → label** A reverse mapping to convert model predictions back into text labels for the final submission file.\n",
    "These mappings were passed into the model configuration to ensure consistency across training and inference.\n",
    "\n",
    "#### **1.1.6 Summary of Preprocessing Results**\n",
    "After preprocessing, we achieved the following:\n",
    "- Loaded three heterogeneous data sources (`final_posts.json`, `emotion.csv`, `data_identification.csv`)  \n",
    "- Unified them into a single clean dataset using the **`id`** key  \n",
    "- Split samples into training and test based on the **`split`** column  \n",
    "- Created a stratified train/validation split for stable model evaluation  \n",
    "- Converted emotion labels into numerical IDs suitable for transformer models  \n",
    "Tokenization, padding, truncation, and converting text into tensor features are not considered raw preprocessing. These steps belong to **Feature Engineering / Model Input Preparation**, which will be described in the next section.\n",
    "---\n",
    "### **1.2 Feature Engineering Steps**\n",
    "In this competition, we focused on preparing text features suitable for Transformer-based models rather than manually designing traditional features. All four models used in our system (DistilBERT, RoBERTa-base, RoBERTa-large, and DeBERTa-v3) rely on subword tokenization and standardized input formatting. The feature engineering steps are summarized as follows.\n",
    "#### **1.2.1 Tokenization (Subword Encoding)**\n",
    "Each model requires its own tokenizer:\n",
    "    - DistilBERT → DistilBertTokenizerFast  \n",
    "    - RoBERTa-base / RoBERTa-large → RobertaTokenizerFast  \n",
    "    - DeBERTa-v3 → DebertaV2Tokenizer  \n",
    "The tokenizer converts raw text into subword pieces and then into numerical token IDs that the model can process.\n",
    "#### **1.2.2 Generating Model-Ready Input Features**\n",
    "The tokenizer outputs several essential fields:\n",
    "    - **input_ids** — Token indices representing the text  \n",
    "    - **attention_mask** — A binary mask indicating which tokens are real and which are padding  \n",
    "    - **token_type_ids** — Not used in RoBERTa models  \n",
    "These fields form the complete representation of each post for Transformer models.\n",
    "#### **1.2.3 Padding and Truncation**\n",
    "To ensure uniform sequence length across all inputs and enable efficient GPU batching, we set:\n",
    "    - `max_length = 128 `\n",
    "    - `padding=\"max_length\"` \n",
    "    - `truncation = True`  \n",
    "This means all posts are either padded or truncated to a fixed length of 128 tokens, which balances coverage and computational efficiency.\n",
    "#### **1.2.4 Formatting the Tokenized Dataset**\n",
    "After tokenization, we convert the processed text into PyTorch tensors so that the HuggingFace Trainer can directly use them. For each data split (train, validation, test), we generate:\n",
    "    - `input_ids `\n",
    "    - `attention_mask`  \n",
    "    - `label` (train and validation only)\n",
    "This creates a clean, unified structure for model training and inference.\n",
    "#### **1.2.5 Label Encoding**\n",
    "Emotion categories must be converted into numerical labels.  \n",
    "We create two mapping dictionaries:\n",
    "- **`label2id`** — Converts label names into numeric IDs  \n",
    "- **`id2label`** — Converts model predictions back into human-readable labels  \n",
    "These mappings ensure consistency throughout preprocessing, training, and final submission.\n",
    "#### **1.2.6 Probability-Level Feature Construction for Ensemble**\n",
    "Although not a traditional feature engineering technique, we also constructed model-level features to support ensemble learning:\n",
    "    - Convert each model’s logits into softmax probabilities  \n",
    "    - Combine predictions from four models using weighted soft voting  \n",
    "    - Tune ensemble weights to maximize Macro-F1 on validation data  \n",
    "This probability-level fusion serves as a high-level feature engineering step and contributed significantly to improving the final competition performance.\n",
    "\n",
    "- Overall, our feature engineering approach is fully aligned with modern Transformer workflows—prioritizing clean text-to-token processing and probability-based fusion rather than manual handcrafted features.\n",
    "\n",
    "---\n",
    "### **1.3 Explanation of Your Model**\n",
    "\n",
    "Our final solution uses a **multi-model Transformer ensemble**, which combines the strengths of several architectures using **soft voting**. This design significantly improves prediction stability and overall F1 performance compared to any individual model.\n",
    "\n",
    "#### **1.3.1 Individual Models**\n",
    "We trained **four different Transformer-based classifiers**, each contributing unique strengths:\n",
    "**1. **DistilBERT (distilbert-base-uncased)**\n",
    "A lightweight and efficient version of BERT.\n",
    "- Fast to train and serves as a strong baseline.\n",
    "- Provides stable probability outputs useful for ensemble smoothing.\n",
    "\n",
    "**2. RoBERTa-base**\n",
    "A widely used, robust model trained on a large corpus.\n",
    "- Strong contextual representation.\n",
    "- One of the best-performing single models on our validation set.\n",
    "\n",
    "**3. RoBERTa-base (Class-Weighted)**\n",
    "To address **class imbalance**, we added class weights in the loss function.\n",
    "- Improves recall for minority emotion classes.\n",
    "- Enhances diversity in the ensemble, which is crucial for soft voting.\n",
    "\n",
    "**4. DeBERTa-v3-base**\n",
    "Uses Disentangled Attention and improved position encoding.\n",
    "- Learns fine-grained emotional cues.\n",
    "- Slightly unstable but provides complementary information, so we assign it a smaller ensemble weight.\n",
    "\n",
    "**1.3.2 Soft Voting Ensemble (Probability-Level Combination)**\n",
    "Each model outputs a **probability distribution** across the six emotion classes:\n",
    "\n",
    "$$\n",
    "P^{(m)} = [p^{(m)}_{\\text{anger}}, p^{(m)}_{\\text{disgust}}, p^{(m)}_{\\text{fear}}, p^{(m)}_{\\text{joy}}, p^{(m)}_{\\text{sadness}}, p^{(m)}_{\\text{surprise}}]\n",
    "$$\n",
    "\n",
    "We assign weights to the models as follows:\n",
    "\n",
    "| Model              | Weight |\n",
    "|--------------------|--------|\n",
    "| DistilBERT         | **1.0** |\n",
    "| RoBERTa-base       | **1.0** |\n",
    "| RoBERTa-weighted   | **1.0** |\n",
    "| DeBERTa-v3         | **0.5** |\n",
    "\n",
    "For each class \\(c\\), the ensemble probability is computed as a **weighted sum of probabilities**:\n",
    "\n",
    "$$\n",
    "P_{\\text{final}}(c) =\n",
    "w_{\\text{distil}} \\cdot p^{(distil)}_c +\n",
    "w_{\\text{rob}} \\cdot p^{(rob)}_c +\n",
    "w_{\\text{robw}} \\cdot p^{(robw)}_c +\n",
    "w_{\\text{deb}} \\cdot p^{(deb)}_c\n",
    "$$\n",
    "\n",
    "The final prediction is the class with the highest weighted probability:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_c P_{\\text{final}}(c)\n",
    "$$\n",
    "\n",
    "Soft voting leverages the **confidence levels** from each model, producing more reliable predictions than hard majority voting.\n",
    "\n",
    "**1.3.3 Why the Ensemble Works Better**\n",
    "\n",
    "- Each model captures different linguistic and emotional nuances.\n",
    "- Class-weighted RoBERTa improves minority-class performance.\n",
    "- DeBERTa adds fine-grained representations despite its instability.\n",
    "- Soft voting reduces variance and overfitting by averaging across models.\n",
    "As a result, the **four-model soft voting ensemble** achieved: **Leaderboard Score: 0.7037 — our strongest submission**\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "During model development, we experimented with several approaches to improve performance and understand the behavior of the models.\n",
    "\n",
    "#### 2.1.1 Comparing Different Transformer Architectures\n",
    "We trained and compared:\n",
    "- DistilBERT  \n",
    "- RoBERTa-base  \n",
    "- RoBERTa-base with class weights  \n",
    "- DeBERTa-v3-base  \n",
    "This confirmed that **model diversity** is beneficial: even if one model is not the top individual performer, it can still improve the ensemble when combined appropriately.\n",
    "\n",
    "#### 2.1.2 Hyperparameter Variations\n",
    "We tried:\n",
    "- Different learning rates: `1e-5` vs `2e-5`  \n",
    "- Different maximum sequence lengths: `128` vs `256`  \n",
    "- Different batch sizes: `8` vs `16`\n",
    "The configuration with learning rate **2e-5**, `max_length = 128`, and `batch_size = 8` provided a good trade-off between performance and training time.\n",
    "#### 2.1.3 Checkpoints vs. Clean Training\n",
    "At first, we used frequent checkpoint saving:\n",
    "- This quickly consumed disk space.\n",
    "- It did not consistently yield better final performance.\n",
    "Later, we simplified the process by not saving intermediate checkpoints and focusing on a single well-trained version per model. This made the pipeline more stable and easier to manage.\n",
    "\n",
    "#### 2.1.4 Different Ensemble Strategies\n",
    "We compared:\n",
    "- **Hard voting** (majority vote on predicted labels)  \n",
    "- **Unweighted soft voting** (simple average of probabilities)  \n",
    "- **Weighted soft voting** (weighted average of probabilities)\n",
    "Weighted soft voting provided the **best macro F1**, because it allows us to:\n",
    "- Give more importance to strong, stable models.\n",
    "- Still leverage the complementary strengths of weaker but diverse models.\n",
    "---\n",
    "### 2.2 Mention Insights You Gained\n",
    "#### 2.2.1 Diversity Is More Valuable Than a Single “Best” Model\n",
    "Even though RoBERTa-base was a strong single model, the ensemble of four different architectures consistently performed better. This supports the idea that **different models capture different aspects** of the data.\n",
    "\n",
    "#### 2.2.2 Handling Class Imbalance Is Crucial for Macro F1\n",
    "The class-weighted RoBERTa model improved performance on minority classes, which directly boosted macro F1. This shows that:\n",
    "- Improving under-represented classes can be more beneficial than squeezing small gains from the majority class.\n",
    "\n",
    "#### 2.2.3 DeBERTa Adds Useful Semantic Signals\n",
    "DeBERTa-v3-base sometimes behaved less stably as a single model, but in the ensemble, its additional semantic information helped improve prediction quality. Assigning it a smaller weight (0.5) struck a good balance between **diversity** and **stability**.\n",
    "\n",
    "#### 2.2.4 Simpler Pipelines Are Easier to Control\n",
    "Excessive checkpoint saving made training slower and complicated file management without clear gains. A simpler pipeline with:\n",
    "- Clean data preprocessing  \n",
    "- Fixed hyperparameters  \n",
    "- A small number of robust models  \n",
    "turned out to be more reliable and easier to reproduce.\n",
    "\n",
    "#### 2.2.5 Preprocessing Integrity Matters for Transformer Models\n",
    "Although we did not heavily clean the raw text, careful handling of:\n",
    "- ID alignment  \n",
    "- Correct train / validation / test splits  \n",
    "- Label mapping  \n",
    "was critical. Small mistakes at the preprocessing stage can invalidate the whole training process, especially in supervised NLP competitions.\n",
    "\n",
    "Overall, this project highlighted the importance of:\n",
    "- Careful data integration  \n",
    "- Sensible model selection  \n",
    "- Thoughtful ensemble design  \n",
    "in achieving competitive performance on an emotion classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "DATA_DIR = Path(\"data\") \n",
    "print(\"DATA_DIR:\", DATA_DIR.resolve())\n",
    "\n",
    "# 2. Load final_posts.json (text content)\n",
    "json_path = DATA_DIR / \"final_posts.json\"\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    posts_raw = json.load(f)\n",
    "\n",
    "records = []\n",
    "for item in posts_raw:\n",
    "    post = item[\"root\"][\"_source\"][\"post\"]\n",
    "    records.append({\n",
    "        \"id\": post[\"post_id\"],\n",
    "        \"text\": post[\"text\"]\n",
    "    })\n",
    "\n",
    "df_posts = pd.DataFrame(records)\n",
    "print(\"df_posts:\", df_posts.shape)\n",
    "df_posts.head()\n",
    "\n",
    "# 3. Load identification + emotion files\n",
    "df_id = pd.read_csv(DATA_DIR / \"data_identification.csv\")   # contains: id, split\n",
    "df_emotion = pd.read_csv(DATA_DIR / \"emotion.csv\")          # contains: id, emotion\n",
    "\n",
    "print(\"df_id:\", df_id.shape)\n",
    "print(\"df_emotion:\", df_emotion.shape)\n",
    "\n",
    "\n",
    "# 4. Merge identification with emotion labels\n",
    "df_id_emotion = df_id.merge(df_emotion, on=\"id\", how=\"left\")\n",
    "print(\"df_id_emotion:\", df_id_emotion.shape)\n",
    "\n",
    "\n",
    "# 5. Merge text content\n",
    "df_full = df_id_emotion.merge(df_posts, on=\"id\", how=\"left\")\n",
    "print(\"df_full:\", df_full.shape)\n",
    "df_full.head()\n",
    "\n",
    "\n",
    "# 6. Split into train / test datasets\n",
    "df_train_full = df_full[df_full[\"split\"] == \"train\"].copy()\n",
    "df_test = df_full[df_full[\"split\"] == \"test\"].copy()\n",
    "\n",
    "print(\"Train size:\", df_train_full.shape)\n",
    "print(\"Test size:\", df_test.shape)\n",
    "\n",
    "df_train_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the code related to the feature engineering steps in cells inside this section\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Encode emotion labels\n",
    "label_list = sorted(df_train_full[\"emotion\"].unique())\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "df_train_full = df_train_full.copy()\n",
    "df_train_full[\"label\"] = df_train_full[\"emotion\"].map(label2id)\n",
    "\n",
    "\n",
    "# 2. Convert pandas → HF Dataset\n",
    "train_dataset = Dataset.from_pandas(\n",
    "    df_train_full[[\"text\", \"label\"]],\n",
    "    preserve_index=False\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset.from_pandas(\n",
    "    valid_df[[\"text\", \"label\"]],\n",
    "    preserve_index=False\n",
    ")\n",
    "\n",
    "test_dataset = Dataset.from_pandas(\n",
    "    df_test[[\"text\"]],\n",
    "    preserve_index=False\n",
    ")\n",
    "\n",
    "print(train_dataset)\n",
    "print(valid_dataset)\n",
    "print(test_dataset)\n",
    "\n",
    "\n",
    "# 3. Load Tokenizer (baseline for Feature Engineering demo)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_text(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "\n",
    "# 4. Apply tokenization\n",
    "encoded_train = train_dataset.map(tokenize_text, batched=True)\n",
    "encoded_valid = valid_dataset.map(tokenize_text, batched=True)\n",
    "encoded_test = test_dataset.map(tokenize_text, batched=True)\n",
    "\n",
    "\n",
    "# 5. Set HF Datasets to PyTorch format\n",
    "encoded_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "encoded_valid.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "encoded_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "encoded_train, encoded_valid, encoded_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the code related to the model implementation steps in cells inside this section\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 1. Utility\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"macro_f1\": macro_f1}\n",
    "\n",
    "def softmax_np(x):\n",
    "    e = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e / e.sum(axis=-1, keepdims=True)\n",
    "\n",
    "# 2. Load Four Models Used in Final Ensemble\n",
    "\n",
    "# ------- (1) DistilBERT -------\n",
    "model_distil = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "args_distil = TrainingArguments(\n",
    "    output_dir=\"distilbert_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.0,\n",
    "    save_steps=10**9\n",
    ")\n",
    "trainer_distil = Trainer(\n",
    "    model=model_distil,\n",
    "    args=args_distil,\n",
    "    train_dataset=encoded_train,\n",
    "    eval_dataset=encoded_valid,\n",
    "    tokenizer=tokenizer_bert,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "# ------- (2) RoBERTa-base -------\n",
    "model_rb = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "tokenizer_rb = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "args_rb = TrainingArguments(\n",
    "    output_dir=\"roberta_base_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.0,\n",
    "    save_steps=10**9\n",
    ")\n",
    "trainer_rb = Trainer(\n",
    "    model=model_rb,\n",
    "    args=args_rb,\n",
    "    train_dataset=encoded_train,\n",
    "    eval_dataset=encoded_valid,\n",
    "    tokenizer=tokenizer_rb,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "# ------- (3) RoBERTa-base Weighted -------\n",
    "model_rb_w = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "tokenizer_rb_w = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "class_weights = torch.tensor([1.0, 5.0, 2.0, 1.0, 2.0, 1.0]).to(model_rb_w.device)\n",
    "model_rb_w.config.problem_type = \"single_label_classification\"\n",
    "\n",
    "args_rb_w = TrainingArguments(\n",
    "    output_dir=\"roberta_weighted_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.0,\n",
    "    save_steps=10**9\n",
    ")\n",
    "trainer_rb_w = Trainer(\n",
    "    model=model_rb_w,\n",
    "    args=args_rb_w,\n",
    "    train_dataset=encoded_train,\n",
    "    eval_dataset=encoded_valid,\n",
    "    tokenizer=tokenizer_rb_w,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ------- (4) DeBERTa-v3-base -------\n",
    "model_deb = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v3-base\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "tokenizer_deb = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "args_deb = TrainingArguments(\n",
    "    output_dir=\"deberta_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=10**9\n",
    ")\n",
    "trainer_deb = Trainer(\n",
    "    model=model_deb,\n",
    "    args=args_deb,\n",
    "    train_dataset=encoded_train,\n",
    "    eval_dataset=encoded_valid,\n",
    "    tokenizer=tokenizer_deb,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "# 3. Get Validation & Test Predictions\n",
    "\n",
    "pred_valid_distil = trainer_distil.predict(encoded_valid).predictions\n",
    "pred_valid_rb = trainer_rb.predict(encoded_valid).predictions\n",
    "pred_valid_rb_w = trainer_rb_w.predict(encoded_valid).predictions\n",
    "pred_valid_deb = trainer_deb.predict(encoded_valid).predictions\n",
    "\n",
    "pred_test_distil = trainer_distil.predict(encoded_test).predictions\n",
    "pred_test_rb = trainer_rb.predict(encoded_test).predictions\n",
    "pred_test_rb_w = trainer_rb_w.predict(encoded_test).predictions\n",
    "pred_test_deb = trainer_deb.predict(encoded_test).predictions\n",
    "\n",
    "\n",
    "# Convert logits → probabilities\n",
    "probs_valid_distil = softmax_np(pred_valid_distil)\n",
    "probs_valid_rb = softmax_np(pred_valid_rb)\n",
    "probs_valid_rb_w = softmax_np(pred_valid_rb_w)\n",
    "probs_valid_deb = softmax_np(pred_valid_deb)\n",
    "\n",
    "probs_test_distil = softmax_np(pred_test_distil)\n",
    "probs_test_rb = softmax_np(pred_test_rb)\n",
    "probs_test_rb_w = softmax_np(pred_test_rb_w)\n",
    "probs_test_deb = softmax_np(pred_test_deb)\n",
    "\n",
    "\n",
    "# 4. Soft-Voting Ensemble (Tuned Weights)\n",
    "\n",
    "w_distil = 1.0\n",
    "w_rb = 1.0\n",
    "w_rb_w = 1.0\n",
    "w_deb = 1.0\n",
    "\n",
    "probs_valid_ens = (\n",
    "    w_distil * probs_valid_distil +\n",
    "    w_rb * probs_valid_rb +\n",
    "    w_rb_w * probs_valid_rb_w +\n",
    "    w_deb * probs_valid_deb\n",
    ") / (w_distil + w_rb + w_rb_w + w_deb)\n",
    "\n",
    "probs_test_ens = (\n",
    "    w_distil * probs_test_distil +\n",
    "    w_rb * probs_test_rb +\n",
    "    w_rb_w * probs_test_rb_w +\n",
    "    w_deb * probs_test_deb\n",
    ") / (w_distil + w_rb + w_rb_w + w_deb)\n",
    "\n",
    "y_valid_pred = np.argmax(probs_valid_ens, axis=-1)\n",
    "y_test_pred = np.argmax(probs_test_ens, axis=-1)\n",
    "\n",
    "print(\"Final validation macro-F1:\", f1_score(y_valid_true, y_valid_pred, average=\"macro\"))\n",
    "\n",
    "# 5. Create Submission File\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": df_test[\"id\"],\n",
    "    \"EMOTION\": [id2label[int(i)] for i in y_test_pred]\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_ensemble_soft_tuned.csv\", index=False)\n",
    "print(\"Saved submission_ensemble_soft_tuned.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025-Lab2-Exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
